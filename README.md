# Overview

>'80% of time in data science and analysis is spent data cleaning'

This includes:</br>
• Loading multiple sources of data</br>
• Consolidating data for analysis</br>
• Reshaping and joining datasets</br>
• Dealing with missing values, duplicates and outliers</br>
• Cleaning strings


# Why is data cleaning important?

• Estimated $3 trillion US GDP lost in 2016 - IBM</br>
• 1 in 3 business leaders did not trust the data sources  used in decision-making</br>

> *'Garbage in leads to garbage out'*

## *You get to know your data*:
• Understanding data through inital cleaning and exploration</br>
• Reduces the risk of incorrect assumptions</br>
• Raises relevant questions</br>
• Discovery of issues such as biases in data collection</br>
• Opportunities to problem solve for unique datasets</br>
• Setup to extract additional insight</br>
• Setup to emphasis particular questions</br>


# Project overview

## Tasks

This project centres around cleaning six dirty datasets [[Folder]](https://github.com/ThisIsJohnnyLau/dirty_data_project)
  
• Task 1 - Decathlon Events [[Analysis]](https://thisisjohnnylau.github.io/task_1.html)  
• Task 2 - Cake Ingredients  [[Analysis]](https://thisisjohnnylau.github.io/task_2.html)  
• Task 3 - Seabirds Spottings  [[Analysis]](https://thisisjohnnylau.github.io/task_3.html)  
• Task 4 - Sweeties Survey  [[Analysis]](https://thisisjohnnylau.github.io/task_4.html)  
• Task 5 - Right Wing Authoritarianism  [[Analysis]](https://thisisjohnnylau.github.io/task_5.html)  
• Task 6 - Dogs [[Analysis]](https://thisisjohnnylau.github.io/task_6.html)  

## Format

Each solution includes:</br>
• Cleaning script</br>
• Commentary, assumptions and process</br>
• Answers to questions</br>

## Required libraries

• here</br> 
• janitor</br> 
• readr</br> 
• tidyverse</br>

## Folder structure 

```
raw_data
data_cleaning_scripts
clean_data
documentation_and_analysis
```